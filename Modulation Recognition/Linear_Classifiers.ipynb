{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assign4_20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo7RQElygVTI"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer as LabelBinarize\n",
        "import _pickle as pickle\n",
        "from sklearn import model_selection\n",
        "from sklearn import linear_model\n",
        "import gc\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn import metrics \n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ks17ck3eEPC"
      },
      "source": [
        "**Download Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqTFFmMFPxak"
      },
      "source": [
        "import requests\r\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmXRSkamQV8R",
        "outputId": "e089c3d3-36ac-4ac7-da49-bf8eb6a05136"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT-vrf6NQYHM"
      },
      "source": [
        "def download_file(url):\r\n",
        "    r = requests.get(url, stream = True)\r\n",
        "    local_filename = url.split('/')[-1]\r\n",
        "    with open(os.path.join(\"/content/drive/My Drive\",local_filename), \"wb\") as file:\r\n",
        "      for block in r.iter_content(chunk_size = 1024):\r\n",
        "        if block:  \r\n",
        "          file.write(block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we4sAn0yQgbu"
      },
      "source": [
        "download_file(\"http://opendata.deepsig.io/datasets/2016.10/RML2016.10b.tar.bz2\")\n",
        "!tar -xf  'drive/My Drive/AssignFour/RML2016.10b.tar.bz2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brY_HqvBUmu3"
      },
      "source": [
        "file_path = 'drive/My Drive/AssignFour/RML2016.10b.dat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QawbIv1Qd-uy",
        "outputId": "6c86e76e-d116-431b-902d-0fc4b85ea7ad"
      },
      "source": [
        "!ls 'drive/My Drive/AssignFour'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comb_decision_tree.sav\traw_logistic.sav  RML2016.10b.tar.bz2\n",
            "comb_logistic.sav\tRML2016.10b.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrHAT6R2ejLx"
      },
      "source": [
        "**Read Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhvKVx7LeXQL",
        "outputId": "ccd17422-916f-4a82-ef94-a243687a8dcd"
      },
      "source": [
        "openedFile = open(file_path,'rb')\n",
        "data = pickle.load(openedFile, encoding='latin1')\n",
        "print(data[('QPSK', 2)].shape)\n",
        "print(data[('PAM4', 8)].shape)\n",
        "keys_list = list(data.keys())\n",
        "temp_data = []\n",
        "label_data = []\n",
        "\n",
        "for i in range(len(keys_list)):\n",
        "    curr_item = data[keys_list[i]] \n",
        "    temp_data.append(curr_item)\n",
        "    for j in range(curr_item.shape[0]):\n",
        "        label_data.append(keys_list[i])\n",
        "        \n",
        "\n",
        "data = np.array(temp_data).reshape(1200000,2,128)\n",
        "\n",
        "labels = np.array(label_data)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "print(labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6000, 2, 128)\n",
            "(6000, 2, 128)\n",
            "(1200000, 2, 128)\n",
            "(1200000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UINYnp5f5tbv"
      },
      "source": [
        "# **Classifiers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YatZJfIA5yp5"
      },
      "source": [
        "**Logistic Regression Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syih_zkw52LH"
      },
      "source": [
        "def logistic_regression(train_x, train_l, validate_x, validate_l, test_x, test_l, filename):\n",
        "  lm = linear_model.LogisticRegression(max_iter=500)\n",
        "  lm.fit(train_x, np.argmax(train_l, axis=1))\n",
        "  print(\"Validation of Logistic Regression: \")\n",
        "  print(lm.score(validate_x, np.argmax(validate_l, axis=1)))\n",
        "  print(\"Test of Logisitc Regression\")\n",
        "  print(lm.score(test_x, np.argmax(test_l, axis=1)))\n",
        "  filename += \"logistic.sav\"\n",
        "  filename_with_path = 'drive/My Drive/AssignFour/' + filename\n",
        "  pickle.dump(lm, open(filename_with_path, 'wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agpw9iXx7j95"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzH6it5f7ow6"
      },
      "source": [
        "def decision_tree(train_x, train_l, validate_x, validate_l, test_x, test_l, filename):\n",
        "  classifier = DecisionTreeClassifier(max_depth=10)\n",
        "  classifier.fit(train_x, np.argmax(train_l, axis=1))\n",
        "  print(\"Validation of Decision Tree: \")\n",
        "  print(classifier.score(validate_x, np.argmax(validate_l, axis=1)))\n",
        "  print(\"Test of Decision Tree\")\n",
        "  print(classifier.score(test_x, np.argmax(test_l, axis=1)))\n",
        "  filename += \"decision_tree.sav\"\n",
        "  filename_with_path = 'drive/My Drive/AssignFour/' + filename\n",
        "  pickle.dump(classifier, open(filename_with_path, 'wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhvPOtLu_30p"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luCM7Ifh_4-4"
      },
      "source": [
        "def randomforest(train_x, train_l, validate_x, validate_l, test_x, test_l, filename):\n",
        "  forest_model = RandomForestClassifier(random_state=0, max_depth=5)\n",
        "  forest_model.fit(train_x, np.argmax(train_l, axis=1))\n",
        "  print(\"Validation of Random Forest: \")\n",
        "  p = forest_model.score(validate_x, np.argmax(validate_l, axis=1))\n",
        "  print(p)\n",
        "  print(\"Test of Random Forest: \")\n",
        "  p = forest_model.score(test_x, np.argmax(test_l, axis=1))\n",
        "  print(p)\n",
        "  filename += \"randomforst.sav\"\n",
        "  filename_with_path = 'drive/My Drive/AssignFour/' + filename\n",
        "  pickle.dump(forest_model, open(filename_with_path, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEbxYo0WqpSp"
      },
      "source": [
        "# **Feature Spaces**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQEC7WF3tVl"
      },
      "source": [
        "**Raw time series as given (two channels)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAuZfSbFqu3S",
        "outputId": "feeb7b4f-6668-4f5a-b4ad-367b49c280e6"
      },
      "source": [
        "raw_feature = data.copy()\n",
        "raw_feature = raw_feature.reshape(1200000,-1)\n",
        "scaler = preprocessing.StandardScaler().fit(raw_feature)\n",
        "raw_feature_norm = scaler.transform(raw_feature)\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(raw_feature_norm, labels, test_size= 0.3, random_state = 42)\n",
        "X_train_final, X_validate, y_train_final, y_validate = model_selection.train_test_split(X_train, y_train, test_size= 0.05, random_state = 42)\n",
        "\n",
        "lb = LabelBinarize()\n",
        "lb.fit_transform(labels[:,0])\n",
        "y_train_final = lb.transform(y_train_final[:,0])\n",
        "y_validate = lb.transform(y_validate[:,0])\n",
        "y_test = lb.transform(y_test[:,0])\n",
        "\n",
        "del(raw_feature)\n",
        "del(raw_feature_norm)\n",
        "gc.collect()\n",
        "logistic_regression(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"raw_\")\n",
        "decision_tree(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"raw_\")\n",
        "randomforest(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"raw_\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation of Logistic Regression: \n",
            "0.16247619047619047\n",
            "Test of Logisitc Regression\n",
            "0.16101944444444444\n",
            "Validation of Decision Tree: \n",
            "0.2517857142857143\n",
            "Test of Decision Tree\n",
            "0.25361666666666666\n",
            "Validation of Random Forest: \n",
            "0.28095238095238095\n",
            "Test of Random Forest: \n",
            "0.280575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDNTba7Km2ws"
      },
      "source": [
        "**First derivative in time (two channels)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jq4yKf3HCO2",
        "outputId": "859fb19a-39f1-45ad-a2b0-85e8fbdad4f8"
      },
      "source": [
        "data_for_derivative = data.copy()\n",
        "der_data = []\n",
        "for d in data_for_derivative:\n",
        "  d1 = np.gradient(d[0])\n",
        "  d2 = np.gradient(d[1])\n",
        "  d1.reshape(1,-1)\n",
        "  d2.reshape(1,-1)\n",
        "  d3 = np.array([d1 , d2]).reshape(256)\n",
        "  der_data.append(d3)\n",
        "\n",
        "der_data = np.array(der_data)\n",
        "scaler = preprocessing.StandardScaler().fit(der_data)\n",
        "der_data_norm = scaler.transform(der_data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(der_data_norm, labels, test_size= 0.3, random_state = 42)\n",
        "X_train_final, X_validate, y_train_final, y_validate = model_selection.train_test_split(X_train, y_train, test_size= 0.05, random_state = 42)\n",
        "\n",
        "lb = LabelBinarize()\n",
        "lb.fit_transform(labels[:,0])\n",
        "y_train_final = lb.transform(y_train_final[:,0])\n",
        "y_validate = lb.transform(y_validate[:,0])\n",
        "y_test = lb.transform(y_test[:,0])\n",
        "\n",
        "del(der_data)\n",
        "del(der_data_norm)\n",
        "del(data_for_derivative)\n",
        "gc.collect()\n",
        "logistic_regression(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"der_\")\n",
        "decision_tree(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"der_\")\n",
        "randomforest(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"der_\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation of Logistic Regression: \n",
            "0.11547619047619048\n",
            "Test of Logisitc Regression\n",
            "0.11530833333333333\n",
            "Validation of Decision Tree: \n",
            "0.21492857142857144\n",
            "Test of Decision Tree\n",
            "0.21413888888888888\n",
            "Validation of Random Forest: \n",
            "0.19502380952380952\n",
            "Test of Random Forest: \n",
            "0.19260833333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUjCZv5RycNH"
      },
      "source": [
        "\n",
        "\n",
        "**Integral in time (two channels)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjeTRU0UyhaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9680eaa9-2e8e-44c1-f106-a5ab5626d6a6"
      },
      "source": [
        "data_for_integral = data.copy()\n",
        "int_data = []\n",
        "for d in data_for_integral:\n",
        "  d1 = np.cumsum(d[0])\n",
        "  d2 = np.cumsum(d[1])\n",
        "  d1.reshape(1,-1)\n",
        "  d2.reshape(1,-1)\n",
        "  d3 = np.array([d1 , d2]).reshape(256)\n",
        "  int_data.append(d3)\n",
        "\n",
        "int_data = np.array(int_data)\n",
        "scaler = preprocessing.StandardScaler().fit(int_data)\n",
        "int_data_norm = scaler.transform(int_data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(int_data_norm, labels, test_size= 0.3, random_state = 42)\n",
        "X_train_final, X_validate, y_train_final, y_validate = model_selection.train_test_split(X_train, y_train, test_size= 0.05, random_state = 42)\n",
        "\n",
        "lb = LabelBinarize()\n",
        "lb.fit_transform(labels[:,0])\n",
        "y_train_final = lb.transform(y_train_final[:,0])\n",
        "y_validate = lb.transform(y_validate[:,0])\n",
        "y_test = lb.transform(y_test[:,0])\n",
        "\n",
        "del(int_data)\n",
        "del(int_data_norm)\n",
        "del(data_for_integral)\n",
        "gc.collect()\n",
        "logistic_regression(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"integ_\")\n",
        "decision_tree(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"integ_\")\n",
        "randomforest(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"integ_\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation of Logistic Regression: \n",
            "0.16192857142857142\n",
            "Test of Logisitc Regression\n",
            "0.16137222222222222\n",
            "Validation of Decision Tree: \n",
            "0.24183333333333334\n",
            "Test of Decision Tree\n",
            "0.2420138888888889\n",
            "Validation of Random Forest: \n",
            "0.17885714285714285\n",
            "Test of Random Forest: \n",
            "0.17941666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss7UC6B70n75"
      },
      "source": [
        "**combinations of 1,2 and 3. (More channels)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2st6a8p20vXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22957b45-5cff-4cd4-a091-6ecc63d3fbb6"
      },
      "source": [
        "data_for_comb = data.copy()\n",
        "comb_data = []\n",
        "for d in data_for_comb:\n",
        "  d1 = np.cumsum(d[0])\n",
        "  d2 = np.cumsum(d[1])\n",
        "  d1.reshape(1,-1)\n",
        "  d2.reshape(1,-1)\n",
        "  d4 = np.gradient(d[0])\n",
        "  d5 = np.gradient(d[1])\n",
        "  d4.reshape(1,-1)\n",
        "  d5.reshape(1,-1)\n",
        "  d3 = np.array([d[0], d[1], d1 , d2, d4, d5]).reshape(768)\n",
        "  comb_data.append(d3)\n",
        "\n",
        "comb_data = np.array(comb_data)\n",
        "scaler = preprocessing.StandardScaler().fit(comb_data)\n",
        "comb_data_norm = scaler.transform(comb_data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(comb_data_norm, labels, test_size= 0.3, random_state = 42)\n",
        "X_train_final, X_validate, y_train_final, y_validate = model_selection.train_test_split(X_train, y_train, test_size= 0.05, random_state = 42)\n",
        "\n",
        "lb = LabelBinarize()\n",
        "lb.fit_transform(labels[:,0])\n",
        "y_train_final = lb.transform(y_train_final[:,0])\n",
        "y_validate = lb.transform(y_validate[:,0])\n",
        "y_test = lb.transform(y_test[:,0])\n",
        "\n",
        "del(comb_data)\n",
        "del(comb_data_norm)\n",
        "del(data_for_comb)\n",
        "gc.collect()\n",
        "logistic_regression(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"comb_\")\n",
        "decision_tree(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"comb_\")\n",
        "randomforest(X_train_final, y_train_final, X_validate, y_validate, X_test, y_test, \"comb_\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation of Logistic Regression: \n",
            "0.16230952380952382\n",
            "Test of Logisitc Regression\n",
            "0.16098055555555554\n",
            "Validation of Decision Tree: \n",
            "0.24114285714285713\n",
            "Test of Decision Tree\n",
            "0.24228333333333332\n",
            "Validation of Random Forest: \n",
            "0.22235714285714286\n",
            "Test of Random Forest: \n",
            "0.22502777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}